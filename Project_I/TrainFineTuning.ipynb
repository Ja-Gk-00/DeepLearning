{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T18:29:57.967462Z",
     "start_time": "2025-03-22T18:29:55.643673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle\n",
    "from ray.tune.syncer import SyncConfig\n",
    "import shutil\n",
    "\n",
    "from DataObjects import DataLoader\n",
    "from Architectures.SimpleCNN import SimpleCNN\n",
    "from Architectures.OptimalCNN import OptimalCNN\n",
    "from Architectures.StochasticDepthCNN import StochasticDepthCNN\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from utils import save_model, load_model\n"
   ],
   "id": "14eb5efd8df3d0e8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T18:29:57.978986Z",
     "start_time": "2025-03-22T18:29:57.970548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
    "                num_epochs: int = 10, lr: float = 0.001,\n",
    "                device: torch.device = None) -> None:\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion: nn.Module = nn.CrossEntropyLoss()\n",
    "    optimizer: torch.optim.Optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss: float = 0.0\n",
    "        train_correct: int = 0\n",
    "        total_train: int = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = batch.data.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels).item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss / total_train\n",
    "        train_acc = train_correct / total_train\n",
    "\n",
    "        model.eval()\n",
    "        val_loss: float = 0.0\n",
    "        val_correct: int = 0\n",
    "        total_val: int = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch.data.to(device)\n",
    "                labels = batch.labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels).item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / total_val\n",
    "        val_acc = val_correct / total_val\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {avg_train_loss:.4f}, Train acc: {train_acc:.4f} | Val loss: {avg_val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "def infer(model: nn.Module, data_loader: DataLoader,\n",
    "          device: torch.device = None) -> list:\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    predictions: list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = batch.data.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             test_loader: Optional[DataLoader] = None,\n",
    "             device: Optional[torch.device] = None) -> Tuple[float, float]:\n",
    "\n",
    "    if test_loader is None:\n",
    "        test_dir = os.path.join(\"Data\", \"Data_converted\", \"test\")\n",
    "        test_loader = DataLoader(test_dir, batch_size=64, shuffle=True)\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    total_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch.data.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_correct += torch.sum(preds == labels).item()\n",
    "            total_test += labels.size(0)\n",
    "\n",
    "    avg_test_loss = test_loss / total_test\n",
    "    test_acc = test_correct / total_test\n",
    "\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "    return avg_test_loss, test_acc"
   ],
   "id": "63fa2403e114a8f2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T18:29:58.203723Z",
     "start_time": "2025-03-22T18:29:58.082944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup directories and DataLoaders\n",
    "train_dir = os.path.abspath(\"Data/Data_converted/train\")\n",
    "val_dir = os.path.abspath(\"Data/Data_converted/valid\")\n",
    "\n",
    "train_loader = DataLoader(train_dir, batch_size=64, shuffle=True, max_per_class=150)\n",
    "val_loader = DataLoader(val_dir, batch_size=64, shuffle=False, max_per_class=150)"
   ],
   "id": "f8ba0402b7c9ef00",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T18:29:58.212932Z",
     "start_time": "2025-03-22T18:29:58.208276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model_tune(config, train_loader, val_loader, num_epochs=10, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Instantiate the model dynamically\n",
    "    model = OptimalCNN(num_classes=10).to(device)\n",
    "\n",
    "    # Loss function & Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs = batch.data.to(device)\n",
    "            labels = batch.labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels).item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss / total_train\n",
    "        train_acc = train_correct / total_train\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = batch.data.to(device)\n",
    "                labels = batch.labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels).item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / total_val\n",
    "        val_acc = val_correct / total_val\n",
    "\n",
    "        # Log to Ray Tune\n",
    "        tune.report({\"accuracy\": val_acc, \"loss\": avg_val_loss })"
   ],
   "id": "bd1f7fd48f0b32bd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T18:29:58.252653Z",
     "start_time": "2025-03-22T18:29:58.250041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@ray.remote\n",
    "def setup_worker():\n",
    "    \"\"\"Ensure each Ray worker has access to the dataset.\"\"\"\n",
    "    worker_data_path = os.path.join(os.getcwd(), \"worker_data\")\n",
    "\n",
    "    if not os.path.exists(worker_data_path):\n",
    "        os.makedirs(worker_data_path)\n",
    "\n",
    "    # Copy dataset if it doesn’t already exist\n",
    "    source_data_path = os.path.abspath(\"Data/Data_converted\")\n",
    "    if not os.path.exists(os.path.join(worker_data_path, \"Data_converted\")):\n",
    "        shutil.copytree(source_data_path, os.path.join(worker_data_path, \"Data_converted\"))\n",
    "\n",
    "    return f\"Worker setup complete: {worker_data_path}\""
   ],
   "id": "bbb4402fd45f3e19",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T18:29:58.297297Z",
     "start_time": "2025-03-22T18:29:58.294936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": tune.choice([2, 4, 8, 16, 32, 64, 128])\n",
    "}\n",
    "scheduler = ASHAScheduler(metric=\"accuracy\",\n",
    "                          mode=\"max\",\n",
    "                          max_t=10,\n",
    "                          grace_period=2,\n",
    "                          reduction_factor=2)"
   ],
   "id": "ab34f63cf137de13",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T18:29:58.487266Z",
     "start_time": "2025-03-22T18:29:58.353944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dir = os.path.abspath(\"Data/Data_converted/test\")\n",
    "test_loader = DataLoader(test_dir, batch_size=64, shuffle=False)"
   ],
   "id": "27b38a740063ab05",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T18:34:13.653961Z",
     "start_time": "2025-03-22T18:29:58.492281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "# Run this before training\n",
    "ray.get(setup_worker.remote())\n",
    "# Launch hyperparameter search\n",
    "tuner = tune.run(\n",
    "    tune.with_parameters(train_model_tune, train_loader=train_loader, val_loader=val_loader),\n",
    "    config=config,\n",
    "    num_samples=10,  # Number of trials\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "# Get the best configuration\n",
    "best_trial = tuner.get_best_trial(\"accuracy\", mode=\"max\", scope=\"all\")\n",
    "best_config = best_trial.config\n",
    "print(\"Best hyperparameters:\", best_config)"
   ],
   "id": "5497c873a6dd3295",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 19:29:59,313\tINFO worker.py:1852 -- Started a local Ray instance.\n",
      "2025-03-22 19:30:00,015\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-03-22 19:30:25</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:25.38        </td></tr>\n",
       "<tr><td>Memory:      </td><td>18.1/31.3 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None<br>Logical resource usage: 10.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">         lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_tune_a9bde_00000</td><td>RUNNING </td><td>192.168.1.43:88898</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.000260397</td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00001</td><td>RUNNING </td><td>192.168.1.43:88900</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.00391419 </td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00002</td><td>RUNNING </td><td>192.168.1.43:88902</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">0.000175241</td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00003</td><td>RUNNING </td><td>192.168.1.43:88903</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">0.00105326 </td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00004</td><td>RUNNING </td><td>192.168.1.43:88905</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00104536 </td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00005</td><td>RUNNING </td><td>192.168.1.43:88904</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00657373 </td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00006</td><td>RUNNING </td><td>192.168.1.43:88901</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00765738 </td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00007</td><td>RUNNING </td><td>192.168.1.43:88906</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.00684927 </td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00008</td><td>RUNNING </td><td>192.168.1.43:88899</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.000137185</td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00009</td><td>RUNNING </td><td>192.168.1.43:88907</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">0.00200625 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-03-22 19:34:13</td></tr>\n",
       "<tr><td>Running for: </td><td>00:04:13.60        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.2/31.3 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=10<br>Bracket: Iter 8.000: 0.48433333333333334 | Iter 4.000: 0.44633333333333336 | Iter 2.000: 0.31733333333333336<br>Logical resource usage: 1.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  accuracy</th><th style=\"text-align: right;\">   loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_tune_a9bde_00000</td><td>TERMINATED</td><td>192.168.1.43:88898</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.000260397</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        247.957 </td><td style=\"text-align: right;\">  0.506667</td><td style=\"text-align: right;\">1.4842 </td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00001</td><td>TERMINATED</td><td>192.168.1.43:88900</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.00391419 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         53.6121</td><td style=\"text-align: right;\">  0.265333</td><td style=\"text-align: right;\">2.10645</td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00002</td><td>TERMINATED</td><td>192.168.1.43:88902</td><td style=\"text-align: right;\">           4</td><td style=\"text-align: right;\">0.000175241</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        205.745 </td><td style=\"text-align: right;\">  0.471333</td><td style=\"text-align: right;\">1.54122</td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00003</td><td>TERMINATED</td><td>192.168.1.43:88903</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">0.00105326 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        250.899 </td><td style=\"text-align: right;\">  0.486667</td><td style=\"text-align: right;\">1.74761</td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00004</td><td>TERMINATED</td><td>192.168.1.43:88905</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00104536 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        243.51  </td><td style=\"text-align: right;\">  0.484   </td><td style=\"text-align: right;\">1.79236</td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00005</td><td>TERMINATED</td><td>192.168.1.43:88904</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00657373 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         61.9092</td><td style=\"text-align: right;\">  0.28    </td><td style=\"text-align: right;\">2.4336 </td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00006</td><td>TERMINATED</td><td>192.168.1.43:88901</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">0.00765738 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         65.701 </td><td style=\"text-align: right;\">  0.276667</td><td style=\"text-align: right;\">2.12934</td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00007</td><td>TERMINATED</td><td>192.168.1.43:88906</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.00684927 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         65.6468</td><td style=\"text-align: right;\">  0.262   </td><td style=\"text-align: right;\">2.15395</td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00008</td><td>TERMINATED</td><td>192.168.1.43:88899</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">0.000137185</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        115.861 </td><td style=\"text-align: right;\">  0.440667</td><td style=\"text-align: right;\">1.61387</td></tr>\n",
       "<tr><td>train_model_tune_a9bde_00009</td><td>TERMINATED</td><td>192.168.1.43:88907</td><td style=\"text-align: right;\">           8</td><td style=\"text-align: right;\">0.00200625 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        119.48  </td><td style=\"text-align: right;\">  0.383333</td><td style=\"text-align: right;\">1.81545</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 19:34:13,639\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/piotr/ray_results/train_model_tune_2025-03-22_19-30-00' in 0.0041s.\n",
      "2025-03-22 19:34:13,643\tINFO tune.py:1041 -- Total run time: 253.63 seconds (253.60 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.0002603965420459722, 'batch_size': 32}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T18:34:20.353080Z",
     "start_time": "2025-03-22T18:34:13.675709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_model = OptimalCNN(num_classes=10)\n",
    "train_model(best_model, train_loader, val_loader, num_epochs=10, lr=best_config[\"lr\"])"
   ],
   "id": "7e95b2f6010609fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train loss: 2.0566, Train acc: 0.2653 | Val loss: 2.2685, Val acc: 0.2033\n",
      "Epoch 2/10 - Train loss: 1.4425, Train acc: 0.5180 | Val loss: 1.6962, Val acc: 0.3873\n",
      "Epoch 3/10 - Train loss: 1.0205, Train acc: 0.7033 | Val loss: 1.6205, Val acc: 0.4273\n",
      "Epoch 4/10 - Train loss: 0.6930, Train acc: 0.8300 | Val loss: 1.5131, Val acc: 0.4633\n",
      "Epoch 5/10 - Train loss: 0.3994, Train acc: 0.9527 | Val loss: 1.5211, Val acc: 0.4667\n",
      "Epoch 6/10 - Train loss: 0.2119, Train acc: 0.9847 | Val loss: 1.4825, Val acc: 0.4820\n",
      "Epoch 7/10 - Train loss: 0.1126, Train acc: 0.9967 | Val loss: 1.5343, Val acc: 0.4780\n",
      "Epoch 8/10 - Train loss: 0.0632, Train acc: 0.9993 | Val loss: 1.4564, Val acc: 0.5053\n",
      "Epoch 9/10 - Train loss: 0.0418, Train acc: 1.0000 | Val loss: 1.4863, Val acc: 0.5067\n",
      "Epoch 10/10 - Train loss: 0.0288, Train acc: 1.0000 | Val loss: 1.4263, Val acc: 0.5133\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T18:34:44.783847Z",
     "start_time": "2025-03-22T18:34:20.357379Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate(best_model, test_loader)",
   "id": "bd70c89c1d89b297",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9096, Test Accuracy: 0.3955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.9096140019628736, 0.3955111111111111)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
