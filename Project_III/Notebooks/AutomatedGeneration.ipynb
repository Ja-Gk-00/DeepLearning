{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea72a1c4",
   "metadata": {},
   "source": [
    "## More automated approach to training architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ac73e",
   "metadata": {},
   "source": [
    "### Setting up the imports/project path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39b493ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure notebook sees necessary paths\n",
    "import os, sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6c5a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from DataObjects.DataLoader import DataLoader\n",
    "from Architectures.DifussionModel import DiffusionModel\n",
    "from Trainers.DiffusionTrainer import TrainingConfig\n",
    "\n",
    "# Noise schedulers\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import DDPMPipeline\n",
    "\n",
    "# Torch files\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from Utils.utils import make_grid, evaluate\n",
    "\n",
    "from Architectures.DifussionModel import train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2266f260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#Set up the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277006f0",
   "metadata": {},
   "source": [
    "### Setup training objects for diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f74917bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the config\n",
    "config = TrainingConfig().from_json(\"../configs/config1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4953db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data batches: 78\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "Data = DataLoader(config.data_dir, batch_size=config.train_batch_size, shuffle=True, fraction = config.fraction, raw = True, dim_shape=config.image_size)\n",
    "print(f\"Number of data batches: {len(Data)}\")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(Data, batch_size=config.train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=config.num_train_timesteps, beta_start=config.beta_start, beta_end=config.beta_end, beta_schedule=config.beta_schedule, variance_type=config.variance_type)\n",
    "model = DiffusionModel.return_custom_arch(config)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c5bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(Data) * config.num_epochs * config.train_batch_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc2ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Pulpit\\DeepLearning\\DeepLearning\\Project_III\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "Epoch 0:   0%|          | 0/95 [00:00<?, ?it/s]d:\\Pulpit\\DeepLearning\\DeepLearning\\Project_III\\venv\\Lib\\site-packages\\diffusers\\configuration_utils.py:141: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "Epoch 0: 100%|██████████| 95/95 [00:28<00:00,  3.28it/s, loss=0.625, lr=1.9e-5, step=94] \n",
      "Epoch 1: 100%|██████████| 95/95 [00:26<00:00,  3.64it/s, loss=0.0408, lr=3.8e-5, step=189]\n",
      "Epoch 2: 100%|██████████| 95/95 [00:51<00:00,  1.84it/s, loss=0.0165, lr=5.7e-5, step=284] \n",
      "Epoch 3: 100%|██████████| 95/95 [00:27<00:00,  3.48it/s, loss=0.0119, lr=7.6e-5, step=379]\n",
      "Epoch 4: 100%|██████████| 95/95 [00:27<00:00,  3.47it/s, loss=0.0284, lr=9.5e-5, step=474] \n",
      "Epoch 5: 100%|██████████| 95/95 [00:37<00:00,  2.54it/s, loss=0.00365, lr=0.0001, step=569]\n",
      "Epoch 6: 100%|██████████| 95/95 [00:31<00:00,  3.01it/s, loss=0.0709, lr=0.0001, step=664]\n",
      "Epoch 7: 100%|██████████| 95/95 [00:30<00:00,  3.09it/s, loss=0.0034, lr=0.0001, step=759]\n",
      "Epoch 8: 100%|██████████| 95/95 [00:31<00:00,  3.03it/s, loss=0.00891, lr=0.0001, step=854]\n",
      "Epoch 9: 100%|██████████| 95/95 [00:31<00:00,  3.04it/s, loss=0.00839, lr=0.0001, step=949]\n",
      "Epoch 10: 100%|██████████| 95/95 [00:31<00:00,  3.00it/s, loss=0.305, lr=0.0001, step=1044] \n",
      "Epoch 11: 100%|██████████| 95/95 [00:31<00:00,  3.03it/s, loss=0.0108, lr=0.0001, step=1139]\n",
      "Epoch 12: 100%|██████████| 95/95 [00:32<00:00,  2.95it/s, loss=0.0764, lr=0.0001, step=1234]\n",
      "Epoch 13: 100%|██████████| 95/95 [00:36<00:00,  2.61it/s, loss=0.0467, lr=0.0001, step=1329]\n",
      "Epoch 14: 100%|██████████| 95/95 [00:32<00:00,  2.93it/s, loss=0.0467, lr=0.0001, step=1424]\n",
      "Epoch 15: 100%|██████████| 95/95 [00:31<00:00,  2.99it/s, loss=0.00793, lr=0.0001, step=1519]\n",
      "Epoch 16: 100%|██████████| 95/95 [00:32<00:00,  2.97it/s, loss=0.0133, lr=0.0001, step=1614] \n",
      "Epoch 17: 100%|██████████| 95/95 [00:31<00:00,  2.98it/s, loss=0.00266, lr=0.0001, step=1709]\n",
      "Epoch 18: 100%|██████████| 95/95 [00:32<00:00,  2.94it/s, loss=0.00623, lr=0.0001, step=1804]\n",
      "100%|██████████| 1000/1000 [00:55<00:00, 18.00it/s]\n",
      "Epoch 19: 100%|██████████| 95/95 [01:28<00:00,  1.07it/s, loss=0.00287, lr=0.0001, step=1899]\n",
      "Epoch 20: 100%|██████████| 95/95 [00:50<00:00,  1.89it/s, loss=0.097, lr=0.0001, step=1994] \n",
      "Epoch 21: 100%|██████████| 95/95 [00:30<00:00,  3.13it/s, loss=0.0479, lr=0.0001, step=2089]\n",
      "Epoch 22: 100%|██████████| 95/95 [00:30<00:00,  3.16it/s, loss=0.0539, lr=0.0001, step=2184]\n",
      "Epoch 23: 100%|██████████| 95/95 [06:05<00:00,  3.84s/it, loss=0.0024, lr=0.0001, step=2279]\n",
      "Epoch 24: 100%|██████████| 95/95 [00:31<00:00,  3.04it/s, loss=0.00944, lr=0.0001, step=2374]\n",
      "Epoch 25: 100%|██████████| 95/95 [00:31<00:00,  2.99it/s, loss=0.00184, lr=0.0001, step=2469]\n",
      "Epoch 26: 100%|██████████| 95/95 [00:31<00:00,  3.02it/s, loss=0.0165, lr=0.0001, step=2564] \n",
      "Epoch 27: 100%|██████████| 95/95 [00:34<00:00,  2.79it/s, loss=0.000836, lr=0.0001, step=2659]\n",
      "Epoch 28: 100%|██████████| 95/95 [00:31<00:00,  2.99it/s, loss=0.00383, lr=0.0001, step=2754]\n",
      "Epoch 29: 100%|██████████| 95/95 [00:32<00:00,  2.91it/s, loss=0.000866, lr=0.0001, step=2849]\n",
      "Epoch 30: 100%|██████████| 95/95 [00:32<00:00,  2.95it/s, loss=0.00472, lr=0.0001, step=2944]\n",
      "Epoch 31: 100%|██████████| 95/95 [02:26<00:00,  1.55s/it, loss=0.0112, lr=0.0001, step=3039]\n",
      "Epoch 32: 100%|██████████| 95/95 [00:31<00:00,  3.03it/s, loss=0.004, lr=0.0001, step=3134]  \n",
      "Epoch 33: 100%|██████████| 95/95 [00:34<00:00,  2.76it/s, loss=0.00101, lr=0.0001, step=3229]\n",
      "Epoch 34: 100%|██████████| 95/95 [00:43<00:00,  2.19it/s, loss=0.0144, lr=0.0001, step=3324] \n",
      "Epoch 35: 100%|██████████| 95/95 [04:39<00:00,  2.94s/it, loss=0.00409, lr=0.0001, step=3419]\n",
      "Epoch 36: 100%|██████████| 95/95 [00:25<00:00,  3.68it/s, loss=0.000979, lr=0.0001, step=3514]\n",
      "Epoch 37: 100%|██████████| 95/95 [00:23<00:00,  4.01it/s, loss=0.00311, lr=0.0001, step=3609]\n",
      "Epoch 38: 100%|██████████| 95/95 [00:23<00:00,  4.08it/s, loss=0.00457, lr=0.0001, step=3704]\n",
      "100%|██████████| 1000/1000 [01:11<00:00, 14.05it/s]\n",
      "Epoch 39: 100%|██████████| 95/95 [01:41<00:00,  1.07s/it, loss=0.205, lr=9.99e-5, step=3799]\n",
      "Epoch 40: 100%|██████████| 95/95 [01:28<00:00,  1.08it/s, loss=0.0884, lr=9.99e-5, step=3894] \n",
      "Epoch 41: 100%|██████████| 95/95 [00:32<00:00,  2.91it/s, loss=0.0412, lr=9.99e-5, step=3989]\n",
      "Epoch 42: 100%|██████████| 95/95 [01:15<00:00,  1.26it/s, loss=0.0458, lr=9.99e-5, step=4084] \n",
      "Epoch 43: 100%|██████████| 95/95 [01:09<00:00,  1.37it/s, loss=0.00207, lr=9.99e-5, step=4179]\n",
      "Epoch 44: 100%|██████████| 95/95 [00:31<00:00,  3.03it/s, loss=0.00757, lr=9.99e-5, step=4274]\n",
      "Epoch 45: 100%|██████████| 95/95 [02:15<00:00,  1.43s/it, loss=0.00138, lr=9.99e-5, step=4369]\n",
      "Epoch 46: 100%|██████████| 95/95 [00:54<00:00,  1.74it/s, loss=0.0131, lr=9.99e-5, step=4464] \n",
      "Epoch 47: 100%|██████████| 95/95 [00:46<00:00,  2.06it/s, loss=0.000755, lr=9.99e-5, step=4559]\n",
      "Epoch 48: 100%|██████████| 95/95 [00:53<00:00,  1.78it/s, loss=0.00321, lr=9.99e-5, step=4654]\n",
      "Epoch 49: 100%|██████████| 95/95 [00:30<00:00,  3.15it/s, loss=0.000518, lr=9.99e-5, step=4749]\n",
      "Epoch 50: 100%|██████████| 95/95 [00:25<00:00,  3.76it/s, loss=0.00371, lr=9.99e-5, step=4844]\n",
      "Epoch 51: 100%|██████████| 95/95 [00:24<00:00,  3.91it/s, loss=0.00827, lr=9.99e-5, step=4939]\n",
      "Epoch 52: 100%|██████████| 95/95 [00:31<00:00,  3.00it/s, loss=0.003, lr=9.99e-5, step=5034]  \n",
      "Epoch 53: 100%|██████████| 95/95 [00:30<00:00,  3.08it/s, loss=0.000855, lr=9.99e-5, step=5129]\n",
      "Epoch 54: 100%|██████████| 95/95 [00:27<00:00,  3.41it/s, loss=0.0114, lr=9.99e-5, step=5224] \n",
      "Epoch 55: 100%|██████████| 95/95 [00:25<00:00,  3.68it/s, loss=0.0034, lr=9.99e-5, step=5319]\n",
      "Epoch 56: 100%|██████████| 95/95 [00:25<00:00,  3.66it/s, loss=0.000913, lr=9.99e-5, step=5414]\n",
      "Epoch 57: 100%|██████████| 95/95 [00:25<00:00,  3.66it/s, loss=0.0026, lr=9.99e-5, step=5509]\n",
      "Epoch 58: 100%|██████████| 95/95 [00:25<00:00,  3.69it/s, loss=0.00352, lr=9.99e-5, step=5604]\n",
      "100%|██████████| 1000/1000 [00:48<00:00, 20.67it/s]\n",
      "Epoch 59: 100%|██████████| 95/95 [01:15<00:00,  1.26it/s, loss=0.178, lr=9.99e-5, step=5699]\n",
      "Epoch 60: 100%|██████████| 95/95 [00:25<00:00,  3.72it/s, loss=0.0785, lr=9.99e-5, step=5794] \n",
      "Epoch 61: 100%|██████████| 95/95 [00:25<00:00,  3.65it/s, loss=0.0372, lr=9.99e-5, step=5889]\n",
      "Epoch 62: 100%|██████████| 95/95 [00:25<00:00,  3.67it/s, loss=0.0397, lr=9.99e-5, step=5984] \n",
      "Epoch 63: 100%|██████████| 95/95 [00:25<00:00,  3.66it/s, loss=0.00194, lr=9.99e-5, step=6079]\n",
      "Epoch 64: 100%|██████████| 95/95 [00:25<00:00,  3.67it/s, loss=0.00612, lr=9.98e-5, step=6174]\n",
      "Epoch 65: 100%|██████████| 95/95 [00:25<00:00,  3.66it/s, loss=0.00119, lr=9.98e-5, step=6269]\n",
      "Epoch 66: 100%|██████████| 95/95 [00:25<00:00,  3.68it/s, loss=0.0109, lr=9.98e-5, step=6364] \n",
      "Epoch 67: 100%|██████████| 95/95 [00:26<00:00,  3.64it/s, loss=0.00082, lr=9.98e-5, step=6459]\n",
      "Epoch 68: 100%|██████████| 95/95 [00:25<00:00,  3.71it/s, loss=0.00245, lr=9.98e-5, step=6554]\n",
      "Epoch 69: 100%|██████████| 95/95 [00:32<00:00,  2.96it/s, loss=0.00048, lr=9.98e-5, step=6649]\n",
      "Epoch 70: 100%|██████████| 95/95 [00:34<00:00,  2.75it/s, loss=0.00297, lr=9.98e-5, step=6744]\n",
      "Epoch 71: 100%|██████████| 95/95 [00:36<00:00,  2.62it/s, loss=0.00644, lr=9.98e-5, step=6839]\n",
      "Epoch 72: 100%|██████████| 95/95 [00:35<00:00,  2.67it/s, loss=0.00237, lr=9.98e-5, step=6934]\n",
      "Epoch 73: 100%|██████████| 95/95 [00:35<00:00,  2.67it/s, loss=0.000727, lr=9.98e-5, step=7029]\n",
      "100%|██████████| 1000/1000 [01:30<00:00, 11.02it/s]7it/s, loss=0.00917, lr=9.98e-5, step=7124]\n",
      "Epoch 74: 100%|██████████| 95/95 [02:06<00:00,  1.33s/it, loss=0.00917, lr=9.98e-5, step=7124]\n"
     ]
    }
   ],
   "source": [
    "## Launch the training\n",
    "from accelerate import notebook_launcher\n",
    "args = (config, model, noise_scheduler, optimizer, Data, lr_scheduler, device)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5e306",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
